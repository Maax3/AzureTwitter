{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e86afbae-e41e-4cce-adbb-1d99cf1831bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Configuración de token SAS para la conexión con Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db91f6c-b00e-4cda-a38c-458fed4f08a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.tweetsfiles.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.tweetsfiles.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.tweetsfiles.dfs.core.windows.net\", \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2024-07-07T22:27:21Z&st=2024-05-04T14:27:21Z&spr=https&sig=hNwdq5gK3xFj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75377729-fbc3-4142-b8a0-4a32996495c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Instalación de dependencias necesarias\n",
    "* Libreria de sentimiento utilizada: https://github.com/sentiment-analysis-spanish/sentiment-spanish/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c354cf6-2a84-4afa-8487-17e9218f10f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /databricks/python3/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn) (1.9.1)\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (1.9.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.18.5 in /databricks/python3/lib/python3.10/site-packages (from scipy) (1.21.5)\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting sentiment-analysis-spanish\n",
      "  Using cached sentiment_analysis_spanish-0.0.25-py3-none-any.whl (30.0 MB)\n",
      "Installing collected packages: sentiment-analysis-spanish\n",
      "Successfully installed sentiment-analysis-spanish-0.0.25\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#%pip install transformers\n",
    "#%pip install torch\n",
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab8fcd38-25e8-4fa4-bbfc-7165a9336b9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a566ee-de1b-4071-8da7-4a32bc13828d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-2542129566031017>, line 7\u001b[0m\n",
       "\u001b[1;32m      5\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#variables con los valores de Azure\u001b[39;00m\n",
       "\u001b[0;32m----> 7\u001b[0m filename \u001b[38;5;241m=\u001b[39m dbutils\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name_origin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
       "\u001b[1;32m      8\u001b[0m routename \u001b[38;5;241m=\u001b[39m dbutils\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_route_databricks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/dbruntime/WidgetHandlerImpl.py:42\u001b[0m, in \u001b[0;36mWidgetsHandlerImpl.get\u001b[0;34m(self, name)\u001b[0m\n",
       "\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n",
       "\u001b[1;32m     37\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns the current value of a widget with give name.\u001b[39;00m\n",
       "\u001b[1;32m     38\u001b[0m \n",
       "\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    :param name: Name of the argument to be accessed\u001b[39;00m\n",
       "\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    :return: Current value of the widget or default value\u001b[39;00m\n",
       "\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
       "\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_notebookArguments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetArgument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entry_point\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetCurrentBindings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:188\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
       "\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    190\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o367.getArgument.\n",
       ": com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named file_name_origin is defined\n",
       "\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:74)\n",
       "\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:266)\n",
       "\tat sun.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\nFile \u001b[0;32m<command-2542129566031017>, line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#variables con los valores de Azure\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m filename \u001b[38;5;241m=\u001b[39m dbutils\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name_origin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m routename \u001b[38;5;241m=\u001b[39m dbutils\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_route_databricks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\nFile \u001b[0;32m/databricks/python_shell/dbruntime/WidgetHandlerImpl.py:42\u001b[0m, in \u001b[0;36mWidgetsHandlerImpl.get\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns the current value of a widget with give name.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    :param name: Name of the argument to be accessed\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    :return: Current value of the widget or default value\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_notebookArguments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetArgument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entry_point\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetCurrentBindings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:188\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o367.getArgument.\n: com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named file_name_origin is defined\n\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:74)\n\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:266)\n\tat sun.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named file_name_origin is defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.types as tipos\n",
    "sc = SparkContext.getOrCreate()\n",
    "#variables con los valores de Azure\n",
    "filename = dbutils.widgets.get('file_name_origin')\n",
    "#routename = dbutils.widgets.get('save_route_databricks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90e54212-3e63-4a2d-9865-618419b8a7ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Selección de datos para sacar la columna del sentimiento y la URL de cada tweet"
   ]
  },


    "\n",
    "\n",
    "#Indicamos el archivo a procesar\n",
    "file_location = f'abfss://rawdata@tweetsfiles.dfs.core.windows.net/processed_dataflow_csv/{filename}'\n",
    "raw_tweets = spark.read.schema(esquema).format(\"csv\").option(\"delimiter\", \"*\").option(\"header\", \"true\").load(file_location).distinct()\n",
    "\n",
    "sentiment_classifier = pipeline(\n",
    "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", \n",
    "    top_k=1\n",
    ")\n",
    "\n",
    "# Definimos la UDF para obtener el sentimiento => return esperado: 'positivo, 0.4254297'\n",
    "def apply_sentiment(text):\n",
    "    texto = str(text)\n",
    "    sentiment = sentiment_classifier(texto)\n",
    "    score_sentiment = sentiment[0][0]['score']\n",
    "    label_sentiment = sentiment[0][0]['label']\n",
    "\n",
    "    if label_sentiment == 'positive' and score_sentiment <= 0.61:\n",
    "        label_sentiment = 'neutral'\n",
    "    return f\"{label_sentiment}, {score_sentiment}\"\n",
    "sentiment_udf = udf(apply_sentiment)\n",
    "\n",
    "\n",
    "# Definimos la UDF para sacar modificar los valores nulos\n",
    "def apply_clear_null(text):\n",
    "    if text is None:\n",
    "        text = \"No especificado\"\n",
    "    return text\n",
    "clear_null_udf = udf(apply_clear_null)\n",
    "\n",
    "tweets = (raw_tweets\n",
    "             .select(\"primaryID\",\"user_id\",\"username\", \"tweet_id\", \"created_at\", \"retweet_count\", \"likes\", \"view_count\", \"hashtags\", \"text\", \"location\", \"temas\", \"latitud\", \"longitud\")\n",
    "             # Sacamos la URL y la eliminamos del texto principal. Ademas, normalizamos el texto eliminando las tildes\n",
    "             .withColumn(\"URL\", fn.regexp_extract(\"text\", r'(http)\\S+', 0))\n",
    "             .withColumn(\"text\", fn.regexp_replace(fn.translate(\"text\", \"áéíóú\", \"aeiou\"), r'(http)\\S+', \"\"))\n",
    "             # Creamos la columna de sentimientos y de ahí, sacamos 2 columnas [score y el valor]\n",
    "             .withColumn(\"Sentimientos\", sentiment_udf('text'))\n",
    "             .withColumn(\"Score\", fn.substring(fn.split(\"Sentimientos\",r',\\s')[1], 0, 4))\n",
    "             .withColumn(\"Sentimientos\", fn.split(\"Sentimientos\", r',\\s')[0])\n",
    "             # Normalizamos los valores nulos\n",
    "             .withColumn(\"location\", clear_null_udf(\"location\"))\n",
    "             .withColumn(\"hashtags\", clear_null_udf(\"hashtags\"))\n",
    "             )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a7c5175-ae90-4f3c-af56-a052b47b3465",
     "showTitle": false,
     "title": ""
    }

   "source": [
    "#### Guardar los datos en el DataLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25d6b40c-a23d-47ec-b956-447e143772c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ruta = f'abfss://outputdata@tweetsfiles.dfs.core.windows.net/{routename}'\n",
    "ruta = f'abfss://outputdata@tweetsfiles.dfs.core.windows.net/lib/{filename}'\n",
    "ruta_temporal = f'abfss://outputdata@tweetsfiles.dfs.core.windows.net/tmp'\n",
    "tweets.repartition(1).write.format(\"CSV\").option(\"header\", \"true\").option(\"delimiter\", \"*\").mode(\"overwrite\").save(ruta_temporal)\n",
    "\n",
    "contenedor = dbutils.fs.ls(ruta_temporal)\n",
    "for fichero in contenedor:\n",
    "  if '.csv' in fichero.name:\n",
    "    file = fichero.name\n",
    "\n",
    "dbutils.fs.cp(f'{ruta_temporal}/{file}', ruta)\n",
    "dbutils.fs.rm(ruta_temporal, True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Analisis-Sentimientos-LLM",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
